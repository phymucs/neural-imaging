{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NIPs with Improved Manipulation Detection Capabilities\n",
    "\n",
    "This notebook implements a simple training procedure which jointly optimizes a forensics network and a full neural imaging pipeline to improve manipulation detection capabilities. The current pipeline optimizes forensic analysis in challenging near-real-world conditions, i.e., after downsampling and JPEG compression which are often employed when posting images online. The overall model architecture is shown in detail below:\n",
    "\n",
    "![manipulation-training](docs/manipulation_detection_training_architecture.png)\n",
    "\n",
    "For more information, please refer to the following papers:\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. P. Korus, N. Memon, *Content Authentication for Neural Imaging Pipelines: End-to-end Optimization of Photo Provenance in Complex Distribution Channels*, CVPR'19, [arxiv:1812.01516](https://arxiv.org/abs/1812.01516) \n",
    "2. P. Korus, N. Memon, *Neural Imaging Pipelines - the Scourge or Hope of Forensics?*, 2019, [arXiv:1902.10707](https://arxiv.org/abs/1902.10707)\n",
    "\n",
    "This notebook was downloaded from: https://github.com/pkorus/neural-imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import imageio\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import pprint\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from collections import deque, OrderedDict \n",
    "from skimage.measure import compare_ssim, compare_psnr, compare_mse\n",
    "\n",
    "# Load my TF models\n",
    "from models import pipelines\n",
    "from models.forensics import FAN\n",
    "from models.jpeg import DJPG\n",
    "\n",
    "# Helper functions\n",
    "from helpers import coreutils, tf_helpers, validation, plotting, dataset\n",
    "\n",
    "# Training functions\n",
    "from training.manipulation import construct_models, train_manipulation_nip\n",
    "\n",
    "# Plotting results\n",
    "from results import display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@> construct_models (nip_model = 'UNet', patch_size = 128, distribution = None, loss_metric = 'L2', kwargs = {'distribution': {'downsampling': 'pool', 'compression': 'dcn', 'compression_params': {'dirname': './data/raw/dcn/twitter_ent10k/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-4.0bpf-S+-H+250.00'}}, 'loss_metric': 'L2'} )\n",
      "NIP network: UNet model [7,760,268 parameters]\n",
      "Channel compression: DCN from ./data/raw/dcn/twitter_ent10k/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-4.0bpf-S+-H+250.00\n",
      "Loaded model: TwitterDCN-8192D/8x8x32-r:soft-codebook-Q-4.0bpf-S+-H+250.00\n",
      "Building CNN for 5 output classes\n",
      "5x5 conv prefilter shape (?, 128, 128, 3)\n",
      "5x5 conv 1 shape (?, 128, 128, 32) + lrelu + 2x2 pool\n",
      "5x5 conv 2 shape (?, 64, 64, 64) + lrelu + 2x2 pool\n",
      "5x5 conv 3 shape (?, 32, 32, 128) + lrelu + 2x2 pool\n",
      "5x5 conv 4 shape (?, 16, 16, 256) + lrelu + 2x2 pool\n",
      "1x1 conv 5 shape (?, 8, 8, 256) + lrelu\n",
      "Final conv shape (?, 8, 8, 256)\n",
      "Feature shape (?, 256)\n",
      "fc shape: (?, 512) + lrelu\n",
      "fc shape: (?, 128) + lrelu\n",
      "out shape: (?, 5) + softmax\n",
      "Forensics network parameters: 1,341,990\n",
      "\n",
      "# TF objects\n",
      "{'compression': <models.compression.TwitterDCN object at 0x138d37160>,\n",
      " 'fan': <models.forensics.FAN object at 0x137ba9080>,\n",
      " 'lambda': <tf.Tensor 'combined_optimization/nip_weight:0' shape=<unknown> dtype=float32>,\n",
      " 'loss': <tf.Tensor 'combined_optimization/add:0' shape=<unknown> dtype=float32>,\n",
      " 'lr': <tf.Tensor 'combined_optimization/learning_rate:0' shape=<unknown> dtype=float32>,\n",
      " 'nip': <models.pipelines.UNet object at 0x137b5fd30>,\n",
      " 'operations': (<tf.Tensor 'unet/y:0' shape=(?, 256, 256, 3) dtype=float32>,\n",
      "                <tf.Tensor 'distribution/sharpen_filter/clip_by_value:0' shape=(?, 256, 256, 3) dtype=float32>,\n",
      "                <tf.Tensor 'distribution/gaussian_filter/clip_by_value:0' shape=(?, 256, 256, 3) dtype=float32>,\n",
      "                <tf.Tensor 'distribution/jpeg/ycbcr_to_rgb/clip_by_value:0' shape=(?, ?, ?, 3) dtype=float32>,\n",
      "                <tf.Tensor 'distribution/resize_images_1/ResizeBilinear:0' shape=(?, ?, ?, 3) dtype=float32>),\n",
      " 'opt': <tf.Operation 'combined_optimization/opt_combined' type=NoOp>,\n",
      " 'sess': <tensorflow.python.client.session.Session object at 0x137b5fa90>}\n",
      "\n",
      "# TF distribution channel\n",
      "{'compression': 'dcn',\n",
      " 'compression_params': {'dirname': './data/raw/dcn/twitter_ent10k/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-4.0bpf-S+-H+250.00'},\n",
      " 'downsampling': 'pool',\n",
      " 'forensics_classes': ['native', 'sharpen', 'gaussian', 'jpg', 'resample']}\n"
     ]
    }
   ],
   "source": [
    "nip_model = 'UNet'\n",
    "\n",
    "# Define the distribution channel\n",
    "distribution = {\n",
    "    'downsampling': 'pool',\n",
    "    'compression': 'dcn',\n",
    "    'compression_params': {\n",
    "        'dirname': './data/raw/dcn/twitter_ent10k/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-4.0bpf-S+-H+250.00',\n",
    "    }\n",
    "}\n",
    "\n",
    "# Construct the TF model\n",
    "tf_ops, distribution = construct_models(nip_model, distribution=distribution, loss_metric='L2')\n",
    "\n",
    "print('\\n# TF objects')\n",
    "pprint.pprint(tf_ops)\n",
    "\n",
    "print('\\n# TF distribution channel')\n",
    "pprint.pprint(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show tensorboard visualization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tf_helpers\n",
    "tf_helpers.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (A): Show photo manipulation variants BEFORE content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch\n",
    "\n",
    "camera_name = 'Nikon D90'\n",
    "patch_size = 128\n",
    "\n",
    "# Load the camera model\n",
    "tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "\n",
    "# Load a sample image and cut a random patch\n",
    "sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['operations'], feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "fig = plotting.imsc(y_patches, ['{}: {{}}'.format(x) for x in distribution['forensics_classes']], ncols=len(distribution['forensics_classes']), figwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (B): Show photo manipulation variants AFTER content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch after the distribution channel\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "fig = plotting.imsc(y_patches, ['{}+chan. {{}}'.format(x) for x in distribution['forensics_classes']], ncols=len(distribution['forensics_classes']), figwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up training variables\n",
    "\n",
    "# Output directory\n",
    "root_dir = './data/raw/train_manipulation_out/'\n",
    "\n",
    "# Training setup\n",
    "training = {\n",
    "    'camera_name': 'Nikon D90',\n",
    "    'use_pretrained_nip': True,\n",
    "    'patch_size': 128,\n",
    "    'batch_size': 20,\n",
    "    'sampling_rate': 50,\n",
    "    'n_epochs': 1001,\n",
    "    'learning_rate': 1e-4,\n",
    "    'run_number': 0,\n",
    "    'nip_weight': 0.1,\n",
    "    'n_images': 40,\n",
    "    'v_images': 40,\n",
    "    'val_n_patches': 1\n",
    "}\n",
    "\n",
    "# Load the dataset for the given camera\n",
    "data_directory = os.path.join('./data/raw/nip_training_data/', camera_name)\n",
    "\n",
    "# Load training / validation data\n",
    "if 'data' not in globals() or not hasattr(data, 'camera') or data.camera != camera_name:\n",
    "    # Load training / validation data\n",
    "    data = dataset.IPDataset(data_directory, n_images=training['n_images'], v_images=training['v_images'], load='xy', val_rgb_patch_size=2 * training['patch_size'], val_n_patches=training['val_n_patches'])\n",
    "    data.camera = camera_name\n",
    "else:\n",
    "    print('Using pre-loaded data for {}'.format(data.camera))\n",
    "\n",
    "# Set up an example training directory (will be replaced once training is completed)\n",
    "output_directory = os.path.join(root_dir, training['camera_name'], nip_model, 'lr-0.1000/000/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual training\n",
    "output_directory = train_manipulation_nip(tf_ops, training, distribution, data, {'root': root_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion Matrix (Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If not trained, load the NIP and FAN models from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not trained, load the NIP and FAN models from a checkpoint\n",
    "tf_ops['nip'].load_model(os.path.join(output_directory, nip_model, camera_name))\n",
    "tf_ops['fan'].load_model(os.path.join(output_directory, 'FAN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.validation import confusion\n",
    "\n",
    "# Create a function which generates labels for each batch\n",
    "def batch_labels(batch_size, n_classes):\n",
    "    return np.concatenate([x * np.ones((batch_size,), dtype=np.int32) for x in range(n_classes)])\n",
    "\n",
    "n_classes = len(distribution['forensics_classes'])\n",
    "\n",
    "conf_mat = confusion(tf_ops['fan'], data, lambda x: batch_labels(x, n_classes))\n",
    "\n",
    "print(conf_mat.round(2))\n",
    "print('\\nAccuracy: {:.2f}'.format(np.mean(np.diag(conf_mat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Training Progress and Confusion Matrix (from Cached Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSpec(object):\n",
    "    \n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot\n",
    "        self.nips = [type(tf_ops['nip']).__name__]\n",
    "        self.cameras = [training['camera_name']]\n",
    "        self.dir = root_dir\n",
    "        self.regularization = ['lr-{:.4f}'.format(training['nip_weight'])]\n",
    "        self.df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('progress'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('confusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Image Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a sample image and crop a random patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare FFT spectra of an RGB patch developed by a pre-trained and an optimized NIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nip_compare import fft_log_norm, nm\n",
    "\n",
    "# Load the pre-trained NIP model\n",
    "tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "y_patches = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Load the optimized NIP model\n",
    "tf_ops['nip'].load_model(os.path.join(output_directory))\n",
    "y_patches_opt = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "fft_diff = nm(fft_log_norm(np.abs(y_patches_opt.squeeze() - y_patches.squeeze())))\n",
    "\n",
    "fig = plotting.imsc([y_patches, y_patches_opt, fft_diff], ['(A) RGB patch (pre-trained NIP)', '(B) RGB patch (optimized NIP)', 'FFT(|A-B|)'], ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FAN Prediction on a Sample Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify different post-processed variations of a sample patch\n",
    "\n",
    "# Load the NIP and FAN models\n",
    "tf_ops['nip'].load_model(output_directory)\n",
    "tf_ops['fan'].load_model(output_directory)\n",
    "\n",
    "# Fetch processed & distributed patches - as seen by the FAN\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Run the patches through the FAN\n",
    "predicted_class, confidence = tf_ops['fan'].process_direct(y_patches, with_confidence=True)\n",
    "\n",
    "# Prepare labels: real_class -> predicted_class (confidence)\n",
    "labels = ['{} -> {} ({:.2f})'.format(distribution['forensics_classes'][real_class], distribution['forensics_classes'][pred_class], conf) for real_class, (pred_class, conf) in enumerate(zip(predicted_class, confidence))]\n",
    "\n",
    "# Plot all images\n",
    "fig = plotting.imsc(y_patches, labels)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
