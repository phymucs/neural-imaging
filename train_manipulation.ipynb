{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NIPs with Improved Manipulation Detection Capabilities\n",
    "\n",
    "This notebook implements a simple training procedure which jointly optimizes a forensics network and a full neural imaging pipeline to improve manipulation detection capabilities. The current pipeline optimizes forensic analysis in challenging near-real-world conditions, i.e., after downsampling and JPEG compression which are often employed when posting images online. The overall model architecture is shown in detail below:\n",
    "\n",
    "![manipulation-training](docs/manipulation_detection_training_architecture.png)\n",
    "\n",
    "For more information, please refer to the following papers:\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. P. Korus, N. Memon, *Content Authentication for Neural Imaging Pipelines: End-to-end Optimization of Photo Provenance in Complex Distribution Channels*, CVPR'19, [arxiv:1812.01516](https://arxiv.org/abs/1812.01516) \n",
    "2. P. Korus, N. Memon, *Neural Imaging Pipelines - the Scourge or Hope of Forensics?*, 2019, [arXiv:1902.10707](https://arxiv.org/abs/1902.10707)\n",
    "\n",
    "This notebook was downloaded from: https://github.com/pkorus/neural-imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# Helper functions\n",
    "from helpers import plotting, dataset\n",
    "\n",
    "# Training functions\n",
    "from training.manipulation import construct_models, train_manipulation_nip\n",
    "\n",
    "# Plotting results\n",
    "from results import display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip_model = 'ONet'\n",
    "trainable = {} \n",
    "\n",
    "manipulations = ['sharpen:0.5', 'gaussian:0.5', 'jpeg:85', 'resample:75', 'awgn:4', 'median:2']\n",
    "# manipulations = ['sharpen', 'gaussian', 'gamma']\n",
    "\n",
    "# Define the distribution channel\n",
    "# distribution = {\n",
    "#     'downsampling': 'none',\n",
    "#     'compression': 'dcn',\n",
    "#     'compression_params': {\n",
    "#         'dirname': './data/raw/dcn/entropy/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-5.0bpf-S+-H+250.00',        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "distribution = {\n",
    "    'downsampling': 'none',\n",
    "    'compression': 'jpeg',\n",
    "    'compression_params': {\n",
    "        'quality': 50,\n",
    "        'rounding_approximation': 'soft'\n",
    "    }\n",
    "}\n",
    "\n",
    "# distribution = {\n",
    "#     'downsampling': 'none',\n",
    "#     'compression': 'none',\n",
    "#     'compression_params': {}\n",
    "# }\n",
    "\n",
    "# Construct the TF model\n",
    "tf_ops, distribution = construct_models(nip_model, patch_size=64, trainable=trainable, \n",
    "                                        distribution=distribution, manipulations=manipulations, \n",
    "                                        loss_metric='L2')\n",
    "\n",
    "print('\\n# TF objects')\n",
    "pprint.pprint(tf_ops)\n",
    "\n",
    "print('\\n# TF distribution channel')\n",
    "pprint.pprint(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show tensorboard visualization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tf_helpers\n",
    "tf_helpers.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (A): Show photo manipulation variants BEFORE content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ops['fan'].init()\n",
    "probs, loss = tf_ops['sess'].run([tf_ops['fan'].y_, tf_ops['loss']], feed_dict={tf_ops['nip'].x: sample_x, \n",
    "                                                     tf_ops['fan'].y: np.arange(0,8),\n",
    "                                                     tf_ops['lambda_dcn']: 0.0,\n",
    "                                                     tf_ops['lambda_nip']: 0.1,\n",
    "                                                    })\n",
    "print(loss)\n",
    "print(probs.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "sns.set('paper', font_scale=1, style=\"darkgrid\")\n",
    "sns.set_context(\"paper\")\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "\n",
    "rc('axes', titlesize=14)\n",
    "rc('axes', labelsize=14)\n",
    "rc('xtick', labelsize=8)\n",
    "rc('ytick', labelsize=8)\n",
    "rc('legend', fontsize=10)\n",
    "rc('figure', titlesize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch\n",
    "\n",
    "camera_name = 'D90'\n",
    "patch_size = 128\n",
    "\n",
    "# Load the camera model\n",
    "if tf_ops['nip'].count_parameters() > 0:\n",
    "    tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "if 'dirname' in distribution['compression_params']: tf_ops['dcn'].load_model(distribution['compression_params']['dirname'])\n",
    "\n",
    "# Load a sample image and cut a random patch\n",
    "# sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "sample_x = data.next_training_batch(1, 1, 256).squeeze()\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "# sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :]\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['operations'], feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "fig = plotting.imsc(y_patches, ['{} ()'.format(x) for x in distribution['forensics_classes']], \n",
    "                    ncols=len(distribution['forensics_classes']), figwidth=36)\n",
    "\n",
    "fig.axes[0].set_ylabel('Uncompressed')\n",
    "# fig.tight_layout()\n",
    "# fig.subplots_adjust\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "fig.savefig('fig_manipulation_examples_{}.pdf'.format(7), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plotting.sub(len(y_patches), ncols=len(y_patches), figwidth=24)\n",
    "for index, ax in enumerate(axes):\n",
    "    sns.distplot(y_patches[index].reshape((-1,1)), ax=ax, bins=255, kde=False, hist_kws={\"linewidth\": 0.1, \"alpha\": 1, \"color\": \"k\"})\n",
    "#     ax.hist(y_patches[index].reshape((-1,1)), bins=255, range=(0, 1), density=True, color='black', alpha=1)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "fig.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (B): Show photo manipulation variants AFTER content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch after the distribution channel\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "# ['{}+comp.'.format(x) for x in distribution['forensics_classes']]\n",
    "fig = plotting.imsc(y_patches, '', ncols=len(distribution['forensics_classes']), figwidth=24)\n",
    "fig.axes[0].set_ylabel('Compressed')\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "fig.savefig('fig_manipulation_examples_{}c.pdf'.format(7), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up training variables\n",
    "\n",
    "# Output directory\n",
    "root_dir = './data/raw/manipulation_fixed_dcn/'\n",
    "\n",
    "# Training setup\n",
    "training = {\n",
    "    'camera_name': '32k',\n",
    "    'use_pretrained_nip': True,\n",
    "    'patch_size': 64,\n",
    "    'batch_size': 20,\n",
    "    'sampling_rate': 50,\n",
    "    'n_epochs': 0,\n",
    "    'learning_rate': 1e-4,\n",
    "    'run_number': 0,\n",
    "    'lambda_nip': 0.1,\n",
    "    'lambda_dcn': 0.01,\n",
    "    'n_images': 100,\n",
    "    'v_images': 100,\n",
    "    'val_n_patches': 1,\n",
    "    'trainable': trainable\n",
    "}\n",
    "\n",
    "# # Load the dataset for the given camera\n",
    "# data_directory = os.path.join('./data/raw/nip_training_data/', camera_name)\n",
    "\n",
    "# Load training / validation data\n",
    "if 'data' not in globals() or not hasattr(data, 'camera') or data.camera != training['camera_name']:\n",
    "    \n",
    "    # Load the dataset\n",
    "    if nip_model == 'ONet':\n",
    "        # TODO Dirty hack - if the NIP model is the dummy empty model, load RGB images only\n",
    "        data_directory = os.path.join('./data/rgb/', training['camera_name'])\n",
    "        load = 'y'\n",
    "    else:\n",
    "        # Otherwise, load (RAW, RGB) pairs for a specific camera\n",
    "        data_directory = os.path.join('./data/raw/nip_training_data/', training['camera_name'])\n",
    "        load = 'xy'    \n",
    "    \n",
    "    # Load training / validation data\n",
    "    data = dataset.IPDataset(data_directory, n_images=training['n_images'], v_images=training['v_images'], load=load, \n",
    "                             val_rgb_patch_size=2 * training['patch_size'],\n",
    "                             val_n_patches=training['val_n_patches'])\n",
    "    data.camera = training['camera_name']\n",
    "else:\n",
    "    print('Using pre-loaded data for {}'.format(data.camera))\n",
    "\n",
    "# Set up an example training directory (will be replaced once training is completed)\n",
    "output_directory = os.path.join(root_dir, training['camera_name'], nip_model, 'lr-0.1000/000/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual training\n",
    "output_directory = train_manipulation_nip(tf_ops, training, distribution, data, {'root': root_dir})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion Matrix (Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If not trained, load the NIP and FAN models from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not trained, load the NIP and FAN models from a checkpoint\n",
    "# tf_ops['nip'].load_model(os.path.join(output_directory, nip_model, camera_name))\n",
    "tf_ops['fan'].load_model(os.path.join(output_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.validation import confusion\n",
    "\n",
    "# Create a function which generates labels for each batch\n",
    "def batch_labels(batch_size, n_classes):\n",
    "    return np.concatenate([x * np.ones((batch_size,), dtype=np.int32) for x in range(n_classes)])\n",
    "\n",
    "n_classes = len(distribution['forensics_classes'])\n",
    "\n",
    "conf_mat = confusion(tf_ops['fan'], data, lambda x: batch_labels(x, n_classes))\n",
    "\n",
    "print(conf_mat.round(2))\n",
    "print('\\nAccuracy: {:.2f}'.format(np.mean(np.diag(conf_mat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Training Progress and Confusion Matrix (from Cached Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSpec(object):\n",
    "    \n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot\n",
    "        self.nips = [type(tf_ops['nip']).__name__]\n",
    "        self.cameras = [training['camera_name']]\n",
    "        self.dir = root_dir\n",
    "        self.regularization = ['lr-{:.4f}'.format(training['nip_weight'])]\n",
    "        self.df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('progress'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('confusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Image Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a sample image and crop a random patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare FFT spectra of an RGB patch developed by a pre-trained and an optimized NIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nip_compare import fft_log_norm, nm\n",
    "\n",
    "# Load the pre-trained NIP model\n",
    "tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "y_patches = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Load the optimized NIP model\n",
    "tf_ops['nip'].load_model(os.path.join(output_directory))\n",
    "y_patches_opt = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "fft_diff = nm(fft_log_norm(np.abs(y_patches_opt.squeeze() - y_patches.squeeze())))\n",
    "\n",
    "fig = plotting.imsc([y_patches, y_patches_opt, fft_diff], ['(A) RGB patch (pre-trained NIP)', '(B) RGB patch (optimized NIP)', 'FFT(|A-B|)'], ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FAN Prediction on a Sample Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify different post-processed variations of a sample patch\n",
    "\n",
    "# Load the NIP and FAN models\n",
    "tf_ops['nip'].load_model(output_directory)\n",
    "tf_ops['fan'].load_model(output_directory)\n",
    "\n",
    "# Fetch processed & distributed patches - as seen by the FAN\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Run the patches through the FAN\n",
    "predicted_class, confidence = tf_ops['fan'].process_direct(y_patches, with_confidence=True)\n",
    "\n",
    "# Prepare labels: real_class -> predicted_class (confidence)\n",
    "labels = ['{} -> {} ({:.2f})'.format(distribution['forensics_classes'][real_class], distribution['forensics_classes'][pred_class], conf) for real_class, (pred_class, conf) in enumerate(zip(predicted_class, confidence))]\n",
    "\n",
    "# Plot all images\n",
    "fig = plotting.imsc(y_patches, labels)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
