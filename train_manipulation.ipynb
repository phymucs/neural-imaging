{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training NIPs with Improved Manipulation Detection Capabilities\n",
    "\n",
    "This notebook implements a simple training procedure which jointly optimizes a forensics network and a full neural imaging pipeline to improve manipulation detection capabilities. The current pipeline optimizes forensic analysis in challenging near-real-world conditions, i.e., after downsampling and JPEG compression which are often employed when posting images online. The overall model architecture is shown in detail below:\n",
    "\n",
    "![manipulation-training](docs/manipulation_detection_training_architecture.png)\n",
    "\n",
    "For more information, please refer to the following papers:\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. P. Korus, N. Memon, *Content Authentication for Neural Imaging Pipelines: End-to-end Optimization of Photo Provenance in Complex Distribution Channels*, CVPR'19, [arxiv:1812.01516](https://arxiv.org/abs/1812.01516) \n",
    "2. P. Korus, N. Memon, *Neural Imaging Pipelines - the Scourge or Hope of Forensics?*, 2019, [arXiv:1902.10707](https://arxiv.org/abs/1902.10707)\n",
    "\n",
    "This notebook was downloaded from: https://github.com/pkorus/neural-imaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# Helper functions\n",
    "from helpers import plotting, dataset\n",
    "\n",
    "# Training functions\n",
    "from training.manipulation import construct_models, train_manipulation_nip\n",
    "\n",
    "# Plotting results\n",
    "from results import display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nip_model = 'DNet'\n",
    "trainable = {'dcn'} \n",
    "\n",
    "manipulations = ['sharpen', 'gaussian', 'jpeg', 'resample', 'awgn', 'gamma', 'median']\n",
    "\n",
    "# Define the distribution channel\n",
    "distribution = {\n",
    "    'downsampling': 'none',\n",
    "    'compression': 'dcn',\n",
    "    'compression_params': {\n",
    "        'dirname': './data/raw/dcn/entropy/TwitterDCN-8192D/16x16x32-r:soft-codebook-Q-5.0bpf-S+-H+250.00',        \n",
    "    }\n",
    "}\n",
    "\n",
    "# distribution = {\n",
    "#     'downsampling': 'none',\n",
    "#     'compression': 'jpeg',\n",
    "#     'compression_params': {\n",
    "#         'quality': 30,\n",
    "#         'rounding_approximation': 'soft'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# distribution = {\n",
    "#     'downsampling': 'none',\n",
    "#     'compression': 'none',\n",
    "#     'compression_params': {}\n",
    "# }\n",
    "\n",
    "# Construct the TF model\n",
    "tf_ops, distribution = construct_models(nip_model, patch_size=64, trainable=trainable, \n",
    "                                        distribution=distribution, manipulations=manipulations, \n",
    "                                        loss_metric='L2')\n",
    "\n",
    "print('\\n# TF objects')\n",
    "pprint.pprint(tf_ops)\n",
    "\n",
    "print('\\n# TF distribution channel')\n",
    "pprint.pprint(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show tensorboard visualization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tf_helpers\n",
    "tf_helpers.show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (A): Show photo manipulation variants BEFORE content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch\n",
    "\n",
    "camera_name = 'D90'\n",
    "patch_size = 64\n",
    "\n",
    "# Load the camera model\n",
    "tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "if 'dirname' in distribution['compression_params']: tf_ops['dcn'].load_model(distribution['compression_params']['dirname'])\n",
    "\n",
    "# Load a sample image and cut a random patch\n",
    "sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['operations'], feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "fig = plotting.imsc(y_patches, ['{}: ()'.format(x) for x in distribution['forensics_classes']], ncols=len(distribution['forensics_classes']), figwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation (B): Show photo manipulation variants AFTER content distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the constructed pipeline - show various post-processed versions of a patch after the distribution channel\n",
    "\n",
    "# Run the patch through the network\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Plot the images\n",
    "fig = plotting.imsc(y_patches, ['{}+chan. ()'.format(x) for x in distribution['forensics_classes']], ncols=len(distribution['forensics_classes']), figwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up training variables\n",
    "\n",
    "# Output directory\n",
    "root_dir = './data/raw/manipulation_fixed_dcn/'\n",
    "\n",
    "# Training setup\n",
    "training = {\n",
    "    'camera_name': 'Nikon D90',\n",
    "    'use_pretrained_nip': True,\n",
    "    'patch_size': 64,\n",
    "    'batch_size': 20,\n",
    "    'sampling_rate': 50,\n",
    "    'n_epochs': 0,\n",
    "    'learning_rate': 1e-4,\n",
    "    'run_number': 0,\n",
    "    'lambda_nip': 0.0,\n",
    "    'lambda_dcn': 0.01,\n",
    "    'n_images': 40,\n",
    "    'v_images': 30,\n",
    "    'val_n_patches': 2,\n",
    "    'trainable': trainable\n",
    "}\n",
    "\n",
    "# Load the dataset for the given camera\n",
    "data_directory = os.path.join('./data/raw/nip_training_data/', camera_name)\n",
    "\n",
    "# Load training / validation data\n",
    "if 'data' not in globals() or not hasattr(data, 'camera') or data.camera != camera_name:\n",
    "    # Load training / validation data\n",
    "    data = dataset.IPDataset(data_directory, n_images=training['n_images'], v_images=training['v_images'], load='xy', \n",
    "                             val_rgb_patch_size=2 * training['patch_size'],\n",
    "                             val_n_patches=training['val_n_patches'])\n",
    "    data.camera = camera_name\n",
    "else:\n",
    "    print('Using pre-loaded data for {}'.format(data.camera))\n",
    "\n",
    "# Set up an example training directory (will be replaced once training is completed)\n",
    "output_directory = os.path.join(root_dir, training['camera_name'], nip_model, 'lr-0.1000/000/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_ops['dcn'].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_ops['dcn'].__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual training\n",
    "output_directory = train_manipulation_nip(tf_ops, training, distribution, data, {'root': root_dir})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(training['trainable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_opt = ['fan']\n",
    "joint_opt.extend(sorted(training['trainable']))\n",
    "joint_opt = '+'.join(joint_opt)\n",
    "print(joint_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Confusion Matrix (Online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If not trained, load the NIP and FAN models from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not trained, load the NIP and FAN models from a checkpoint\n",
    "tf_ops['nip'].load_model(os.path.join(output_directory, nip_model, camera_name))\n",
    "tf_ops['fan'].load_model(os.path.join(output_directory, 'FAN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.validation import confusion\n",
    "\n",
    "# Create a function which generates labels for each batch\n",
    "def batch_labels(batch_size, n_classes):\n",
    "    return np.concatenate([x * np.ones((batch_size,), dtype=np.int32) for x in range(n_classes)])\n",
    "\n",
    "n_classes = len(distribution['forensics_classes'])\n",
    "\n",
    "conf_mat = confusion(tf_ops['fan'], data, lambda x: batch_labels(x, n_classes))\n",
    "\n",
    "print(conf_mat.round(2))\n",
    "print('\\nAccuracy: {:.2f}'.format(np.mean(np.diag(conf_mat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Training Progress and Confusion Matrix (from Cached Stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultSpec(object):\n",
    "    \n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot\n",
    "        self.nips = [type(tf_ops['nip']).__name__]\n",
    "        self.cameras = [training['camera_name']]\n",
    "        self.dir = root_dir\n",
    "        self.regularization = ['lr-{:.4f}'.format(training['nip_weight'])]\n",
    "        self.df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('progress'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_results(ResultSpec('confusion'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Image Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load a sample image and crop a random patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = np.load(os.path.join('./data/raw/nip_training_data/', camera_name, 'r47fff40at.npy'))\n",
    "\n",
    "H, W = sample_x.shape[0:2]\n",
    "\n",
    "xx = np.random.randint(0, W - patch_size)\n",
    "yy = np.random.randint(0, H - patch_size)\n",
    "\n",
    "sample_x = np.expand_dims(sample_x, axis=0)\n",
    "sample_x = sample_x[:, yy:yy+patch_size, xx:xx+patch_size, :].astype(np.float32) / (2**16 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare FFT spectra of an RGB patch developed by a pre-trained and an optimized NIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_nip_compare import fft_log_norm, nm\n",
    "\n",
    "# Load the pre-trained NIP model\n",
    "tf_ops['nip'].load_model(os.path.join('./data/raw/nip_model_snapshots', camera_name))\n",
    "y_patches = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Load the optimized NIP model\n",
    "tf_ops['nip'].load_model(os.path.join(output_directory))\n",
    "y_patches_opt = tf_ops['sess'].run(tf_ops['nip'].y, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "fft_diff = nm(fft_log_norm(np.abs(y_patches_opt.squeeze() - y_patches.squeeze())))\n",
    "\n",
    "fig = plotting.imsc([y_patches, y_patches_opt, fft_diff], ['(A) RGB patch (pre-trained NIP)', '(B) RGB patch (optimized NIP)', 'FFT(|A-B|)'], ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FAN Prediction on a Sample Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify different post-processed variations of a sample patch\n",
    "\n",
    "# Load the NIP and FAN models\n",
    "tf_ops['nip'].load_model(output_directory)\n",
    "tf_ops['fan'].load_model(output_directory)\n",
    "\n",
    "# Fetch processed & distributed patches - as seen by the FAN\n",
    "y_patches = tf_ops['sess'].run(tf_ops['fan'].x, feed_dict={tf_ops['nip'].x: sample_x})\n",
    "\n",
    "# Run the patches through the FAN\n",
    "predicted_class, confidence = tf_ops['fan'].process_direct(y_patches, with_confidence=True)\n",
    "\n",
    "# Prepare labels: real_class -> predicted_class (confidence)\n",
    "labels = ['{} -> {} ({:.2f})'.format(distribution['forensics_classes'][real_class], distribution['forensics_classes'][pred_class], conf) for real_class, (pred_class, conf) in enumerate(zip(predicted_class, confidence))]\n",
    "\n",
    "# Plot all images\n",
    "fig = plotting.imsc(y_patches, labels)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
