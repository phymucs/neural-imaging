{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import io\n",
    "import imageio\n",
    "\n",
    "from collections import deque\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from scipy.cluster.vq import vq\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Own libraries and modules\n",
    "from compression import jpeg_helpers\n",
    "from helpers import loading, plotting, utils, summaries, tf_helpers, dataset\n",
    "from models import compression\n",
    "from pyfse import pyfse\n",
    "\n",
    "from training.compression import train_dcn, visualize_distribution, visualize_codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = {\n",
    "    'n_epochs': 1501,\n",
    "    'batch_size': 40,\n",
    "    'patch_size': 128,\n",
    "    'sample_dropout': False,\n",
    "    'learning_rate': 1e-4,\n",
    "    'learning_rate_reduction_schedule': 1000,\n",
    "    'learning_rate_reduction_factor': 0.5,    \n",
    "    'validation_schedule': 100,\n",
    "    'convergence_threshold': 1e-4,\n",
    "    'current_epoch': 0,\n",
    "    'validation_is_training': True,\n",
    "    'augmentation_probs': {\n",
    "        'resize': 0.0,\n",
    "        'flip_h': 0.5,\n",
    "        'flip_v': 0.5\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "n_latent = 1024 * 8\n",
    "n_latent_bytes = 0.1\n",
    "\n",
    "bitmap_size = patch_size * patch_size * 3\n",
    "compression_rate = bitmap_size / (n_latent_bytes * n_latent)\n",
    "compression_bpp = 8 * n_latent * n_latent_bytes / patch_size / patch_size\n",
    "\n",
    "print('Bitmap size: {:,d} bytes'.format(bitmap_size))\n",
    "print('Latent size: {:,}-D'.format(n_latent))\n",
    "print('Latent repr: {:,} bytes'.format(n_latent * n_latent_bytes))\n",
    "print('Compression rate: 1:{}'.format(compression_rate))\n",
    "print('Compression Fi  : {:.2f} bpp'.format(compression_bpp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_directory = os.path.join('./data/raw/nip_training_data/Nikon D90/')\n",
    "\n",
    "data = dataset.IPDataset(data_directory, n_images=5, v_images=5, load='xy', val_rgb_patch_size=128)\n",
    "\n",
    "print('Sampling ref size: ', data.W, 'x', data.H)\n",
    "for key in ['training', 'validation']:\n",
    "    if 'x' in data[key]: print('{}/X : {}'.format(key, data[key]['x'].shape))\n",
    "    if 'y' in data[key]: print('{}/Y : {}'.format(key, data[key]['y'].shape))\n",
    "    if key == 'training':\n",
    "        batch_x, batch_y = data.next_training_batch(0, 5, rgb_patch_size=128)\n",
    "        print('training/batch', 'x', batch_x.shape if batch_x is not None else None, 'y', batch_y.shape if batch_y is not None else None)\n",
    "    if key == 'validation':        \n",
    "        batch_x, batch_y = data.next_validation_batch(0, 5)\n",
    "        print('validation/batch', 'x', batch_x.shape if batch_x is not None else None, 'y', batch_y.shape if batch_y is not None else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_directory = os.path.join('./data/compression/')\n",
    "\n",
    "# data = dataset.IPDataset(data_directory, n_images=16000, v_images=800, load='y', val_rgb_patch_size=training['patch_size'])\n",
    "data = dataset.IPDataset(data_directory, n_images=4000, v_images=200, load='y', val_rgb_patch_size=training['patch_size'])\n",
    "# data = dataset.IPDataset(data_directory, n_images=1000, v_images=50, load='y', val_rgb_patch_size=training['patch_size'])\n",
    "\n",
    "for key in ['training', 'validation']:\n",
    "    print('{} : {}'.format(key, data[key]['y'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample batch from the database\n",
    "if 'batch_id' not in globals():\n",
    "    batch_id = 0\n",
    "    n_batches = data['training']['y'].shape[0] // training['batch_size']\n",
    "    \n",
    "batch_id = (batch_id + 1) % n_batches\n",
    "\n",
    "print('Batch id: {} ({})'.format(batch_id, training['batch_size']))\n",
    "\n",
    "batch_x = data.next_training_batch(batch_id, training['batch_size'], training['patch_size'])\n",
    "fig = plotting.imsc(batch_x[:8], ncols=8, figwidth=25)\n",
    "\n",
    "batch_x = data.next_validation_batch(batch_id, training['batch_size'])\n",
    "fig = plotting.imsc(batch_x[:8], ncols=8, figwidth=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Deep Compression Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResDCN(compression.DCN):\n",
    "    \n",
    "    def construct_model(self):\n",
    "        \n",
    "        with tf.name_scope('dcn'):\n",
    "\n",
    "            activation = tf.nn.ResDCN\n",
    "            last_activation = tf.nn.sigmoid\n",
    "\n",
    "            print('Building Deep Compression Network with d-latent={}'.format(n_latent))\n",
    "\n",
    "            net = self.x\n",
    "            print('net size: {}'.format(net.shape))\n",
    "        \n",
    "            # Convolutions\n",
    "            n_filters = self.n_filters\n",
    "            \n",
    "            net = tf.contrib.layers.conv2d(net, 64, self.kernel, stride=2, activation_fn=activation, scope='dcn{}/conv_{}'.format(self.label, 0))\n",
    "            \n",
    "            for r in range(self.n_layers):\n",
    "                net = tf.contrib.layers.conv2d(net, n_filters, self.kernel, stride=2, activation_fn=activation, scope='dcn{}/conv_{}'.format(self.label, r))\n",
    "            #     print('net size: {}'.format(net.shape))\n",
    "#                     net = tf.contrib.layers.max_pool2d(net, 2, scope='dcn{}/pool_{}'.format(self.label, r))\n",
    "                print('net size: {} // {}'.format(net.shape, net))\n",
    "                n_filters *= self.n_fscale\n",
    "\n",
    "            # Flatten and get latent representation\n",
    "            flat = tf.contrib.layers.flatten(net, scope='dcn{}/flat_{}'.format(self.label, 0))\n",
    "            print('net size: {}'.format(flat.shape))\n",
    "\n",
    "            latent = tf.contrib.layers.fully_connected(flat, self.n_latent, activation_fn=activation, scope='dcn{}/dense_{}'.format(self.label, 0))\n",
    "            print('net size: {}'.format(latent.shape))\n",
    "\n",
    "            inet = tf.contrib.layers.fully_connected(latent, int(flat.shape[-1]), activation_fn=activation, scope='dcn{}/dense_{}'.format(self.label, 1))\n",
    "            print('net size: {}'.format(inet.shape))\n",
    "            inet = tf.reshape(net, tf.shape(net), name='dcn{}/reshape_{}'.format(self.label, 0))\n",
    "            print('net size: {}'.format(inet.shape))\n",
    "\n",
    "            # Transposed convolutions\n",
    "            for r in range(self.n_layers):\n",
    "                inet = tf.contrib.layers.conv2d_transpose(inet, 3 if r == self.n_layers - 1 else n_filters, self.kernel, stride=2, \n",
    "                                                          activation_fn=last_activation if r == self.n_layers - 1 else activation,\n",
    "                                                          scope='dcn{}/tconv_{}'.format(self.label, r))\n",
    "                print('net size: {}'.format(inet.shape))\n",
    "                n_filters = n_filters // self.n_fscale\n",
    "\n",
    "            y = inet\n",
    "\n",
    "        with tf.name_scope('dcn{}_optimization'.format(self.label)):\n",
    "            lr = tf.placeholder(tf.float32, name='dcn_learning_rate')\n",
    "            loss = tf.nn.l2_loss(self.x - y)\n",
    "            adam = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            opt = adam.minimize(loss, var_list=self.parameters)\n",
    "            \n",
    "        return y, lr, loss, adam, opt, latent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DCN instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "dcn = compression.AutoencoderDCN(sess, graph, patch_size=training['patch_size'], n_latent=0, n_filters=8, n_fscale=2, n_layers=2, r_layers=0, \n",
    "            rounding='soft', dropout=False, use_batchnorm=True, train_codebook=False, latent_bpf=5, scale_latent=False, entropy_weight=None)\n",
    "\n",
    "# dcn = TwitterDCN(sess, graph, patch_size=128)\n",
    "\n",
    "print(dcn.summary())\n",
    "print(dcn.model_code)\n",
    "# print(dcn.count_parameters_breakdown())\n",
    "print('Compression stats:', dcn.compression_stats(n_latent_bytes=0.5))\n",
    "\n",
    "# dcn.load_model('./data/raw/compression/aedcn/8x8x192')\n",
    "# dcn.save_model(os.path.join('./data/raw/compression/', dcn.short_name()), 3767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import tf_helpers\n",
    "tf_helpers.show_graph(dcn.graph.as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dcn.init()\n",
    "out = dcn.sess.run(dcn.weights, feed_dict={dcn.x: batch_x})\n",
    "# fig = plotting.imsc(out[0:32, :])\n",
    "\n",
    "with dcn.graph.as_default():\n",
    "    histogram = dcn.sess.run(dcn.histogram, feed_dict={dcn.x: batch_x})\n",
    "    entropy = dcn.sess.run(- tf.reduce_sum(dcn.histogram * tf.log(dcn.histogram)) / 0.6931, feed_dict={dcn.x: batch_x})\n",
    "\n",
    "plt.plot(histogram)\n",
    "print(entropy)\n",
    "\n",
    "print(- np.sum(histogram * np.log2(histogram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dcn({'dcn': dcn}, training, data, './data/raw/compression_playground/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in dcn.parameters:\n",
    "    if np.any(np.isnan(dcn.sess.run(var))):\n",
    "        print('!! NaNs found in {}'.format(var.name))\n",
    "    else:\n",
    "        print('     all ok in {}'.format(var.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.training_step(batch_x, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.save_model(os.path.join('./data/raw/compression/', dcn.name), training['current_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebn = dcn.sess.run(dcn.pre_bn, feed_dict={dcn.x: batch_x})\n",
    "bM = np.mean(prebn, axis=(0,1,2))\n",
    "bV = np.var(prebn, axis=(0,1,2))\n",
    "pM = dcn.sess.run(dcn.graph.get_tensor_by_name('autoencoderdcn/encoder/bn_0/moving_mean:0'))\n",
    "pV = dcn.sess.run(dcn.graph.get_tensor_by_name('autoencoderdcn/encoder/bn_0/moving_variance:0'))\n",
    "print(prebn.shape)\n",
    "print(bM.shape)\n",
    "print(bV.shape)\n",
    "print(pM.shape)\n",
    "print(pV.shape)\n",
    "\n",
    "fig, axes = plotting.sub(2)\n",
    "axes[0].plot(bM, pM, 'o')\n",
    "axes[1].plot(bV, pV, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elements = 8\n",
    "batch_y_bstats = dcn.process(batch_x[:n_elements], is_training=True)\n",
    "batch_y_pstats = dcn.process(batch_x[:n_elements], is_training=False)\n",
    "fig = plotting.imsc(batch_x[:n_elements], ncols=np.max((8, n_elements)), figwidth=25, titles='Input')\n",
    "fig = plotting.imsc(batch_y_bstats[:n_elements], ncols=np.max((8, n_elements)), figwidth=25, titles='is_training=True (bStats)')\n",
    "fig = plotting.imsc(batch_y_pstats[:n_elements], ncols=np.max((8, n_elements)), figwidth=25, titles='is_training=False (pStats)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from helpers import utils\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax = fig.gca()\n",
    "ax.plot(utils.ma_conv(perf['loss']['training'], n=11))\n",
    "# ax.plot(np.arange(0, len(loss['training']), sampling_rate), utils.ma_conv(loss['validation'], n=3))\n",
    "ax.plot(np.arange(0, len(perf['loss']['training']), sampling_rate), perf['loss']['validation'], '-o', alpha=0.3)\n",
    "ax.plot(perf['loss']['training'], '.', alpha=0.1)\n",
    "ax.legend(['train', 'valid'], loc='upper right')\n",
    "# ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.load_model(os.path.join('./data/raw/compression/', dcn.short_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample and a reconstruction of the current batch\n",
    "batch_y = dcn.process(batch_x)\n",
    "f = plotting.imsc(batch_x[0:8], ncols=8, figwidth=20)\n",
    "f = plotting.imsc(batch_y[0:8], ncols=8, figwidth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the codebook\n",
    "\n",
    "def visualize_codebook(dcn):\n",
    "\n",
    "    qmin = -2 ** (dcn.latent_bpf - 1) + 1\n",
    "    qmax = 2 ** (dcn.latent_bpf - 1)\n",
    "\n",
    "    uniform_cbook = np.arange(qmin, qmax + 1)\n",
    "    codebook = dcn.sess.run(dcn.codebook).reshape((-1)).tolist()\n",
    "    print(codebook)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 1))\n",
    "\n",
    "    for x1, x2 in zip(codebook, uniform_cbook):\n",
    "        fig.gca().plot([x1, x2], [0, 1], 'k:')\n",
    "\n",
    "    fig.gca().plot(codebook, np.zeros_like(codebook), 'x')\n",
    "    fig.gca().plot(uniform_cbook, np.ones_like(uniform_cbook), 'ro')\n",
    "    fig.gca().set_ylim([-1, 2])\n",
    "    fig.gca().set_yticks([])\n",
    "    fig.gca().set_xticks(uniform_cbook)\n",
    "    s = io.BytesIO()\n",
    "    fig.savefig(s, format='png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    return imageio.imread(s.getvalue())[:,:,:3]\n",
    "\n",
    "fig = plotting.imsc(visualize_codebook(dcn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore & Understand the Latent Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotting.imsc(visualize_codebook(dcn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(batch_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of the latent features\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "dcn = compression.AutoencoderDCN(sess, graph, patch_size=training['patch_size'], n_latent=0, n_filters=8, n_fscale=2, n_layers=2, r_layers=0, \n",
    "            rounding='soft', dropout=False, use_batchnorm=True, train_codebook=False, latent_bpf=5, scale_latent=False, entropy_weight=None)\n",
    "\n",
    "dcn.init()\n",
    "\n",
    "fig = plotting.imsc(visualize_distribution(dcn, data), figwidth=15)\n",
    "\n",
    "with dcn.graph.as_default():\n",
    "    histogram = dcn.sess.run(dcn.histogram, feed_dict={dcn.x: batch_x, dcn.is_training: True})\n",
    "    entropy = dcn.sess.run(- tf.reduce_sum(dcn.histogram * tf.log(dcn.histogram)) / 0.6931, feed_dict={dcn.x: batch_x, dcn.is_training: True})\n",
    "\n",
    "plt.plot(histogram)\n",
    "print('TF et entropy', entropy)\n",
    "\n",
    "print('NP est entropy', - np.sum(histogram * np.log2(histogram)))\n",
    "\n",
    "bin_centers = np.arange(-32, 32, 0.1)\n",
    "bin_boundaries = np.convolve(bin_centers, [0.5, 0.5], mode='valid')\n",
    "bin_centers = bin_centers[1:-1]\n",
    "\n",
    "batch_z = dcn.compress(batch_x, is_training=True)\n",
    "# Compute frequencies using a histogram\n",
    "# freq = np.histogram(batch_z[:], bins=bin_boundaries, normed=False)[0]\n",
    "hist = np.histogram(batch_z[:], bins=bin_boundaries, density=True)[0]\n",
    "hist = hist.clip(1e-9)\n",
    "probs = hist / np.sum(hist)\n",
    "\n",
    "entropy_emp = np.sum(- probs * np.log2(probs))\n",
    "print('Empirical entropy', entropy_emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_z = dcn.sess.run(dcn.latent_pre, feed_dict={dcn.x: batch_x, dcn.is_training: True})\n",
    "# op = dcn.graph.get_operation_by_name('autoencoderdcn/encoder/conv_0/LeakyRelu')\n",
    "tn = dcn.graph.get_tensor_by_name('autoencoderdcn/encoder/conv_0/weights:0')\n",
    "print(tn.name)\n",
    "batch_z = dcn.sess.run(tn, feed_dict={dcn.x: batch_x, dcn.is_training: True})\n",
    "\n",
    "# batch_z = dcn.compress(batch_x)\n",
    "print(batch_z.shape)\n",
    "print(batch_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of the latent features\n",
    "\n",
    "sample_batch_size = 50\n",
    "batch_x = data.next_validation_batch(0, sample_batch_size)\n",
    "\n",
    "# See latent distribution\n",
    "batch_z = dcn.compress(batch_x)\n",
    "# batch_z = dcn.compress_soft(batch_x)\n",
    "batch_z = batch_z.reshape((sample_batch_size, -1)).T\n",
    "print(batch_z.shape)\n",
    "\n",
    "bin_centers = np.arange(-32, 32, 0.1)\n",
    "bin_boundaries = np.convolve(bin_centers, [0.5, 0.5], mode='valid')\n",
    "bin_centers = bin_centers[1:-1]\n",
    "\n",
    "hist = np.histogram(batch_z[:], bins=bin_boundaries, normed=True)[0]\n",
    "\n",
    "# for bin, value in zip(bin_centers, hist):\n",
    "#     print('{:.3f}'.format(bin), '->', value)   \n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "ax = fig.gca()\n",
    "ax.bar(bin_centers, hist, width=bin_centers[1] - bin_centers[0], color='r')\n",
    "ax.set_title('Histogram of quantized coefficients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an example of soft quantization & compute entropy\n",
    "\n",
    "# Prepare the quantization codebook\n",
    "q_min = np.percentile(batch_z.astype(np.int32), 0.1)\n",
    "q_max = np.percentile(batch_z.astype(np.int32), 100 - 0.1)\n",
    "\n",
    "bin_boundaries = np.arange(q_min + 0.5, q_max + 0.5, 1)\n",
    "bin_centers = np.convolve(bin_boundaries, [0.5, 0.5], mode='valid')\n",
    "\n",
    "print('Bin centers ({}): {}'.format(len(bin_centers), bin_centers.tolist()))\n",
    "\n",
    "batch_ex = batch_z.T[0]\n",
    "value = batch_ex.reshape((4096, 1))\n",
    "\n",
    "# Compute soft quantization\n",
    "sigma = 0.1\n",
    "weights = np.exp(-sigma * np.power(value - bin_centers.reshape(1, len(bin_centers)), 2))\n",
    "weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "soft = np.sum(weights * bin_centers, axis=1)\n",
    "\n",
    "# Compute frequencies using unique\n",
    "indices, _ = vq(batch_z.reshape((-1, )), bin_centers)\n",
    "unique, counts = np.unique(bin_centers[indices], return_counts=True)\n",
    "counts = counts / counts.sum()\n",
    "\n",
    "# Compute soft histogram\n",
    "histogram = np.mean(weights, axis=0).clip(1e-6)\n",
    "histogram = histogram / np.sum(histogram)\n",
    "\n",
    "entropy = - np.sum(counts * np.log2(counts))\n",
    "entropy_estimate = - np.sum(histogram * np.log2(histogram))\n",
    "\n",
    "# Print stats\n",
    "feature_id = 1\n",
    "print('Feature', feature_id)\n",
    "print('Value', value[feature_id])\n",
    "print('Hard Quantization', np.round(value[feature_id]))\n",
    "print('Soft quantization', np.sum(weights[feature_id] * bin_centers))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plotting.sub(2, figwidth=16, figheight=4)\n",
    "\n",
    "axes[0].plot(bin_centers, weights[feature_id])\n",
    "axes[0].set_title('Soft quantization of a selected feature {:.3f} / {:.3f}'.format(batch_ex[feature_id], np.sum(soft[feature_id])))\n",
    "\n",
    "axes[1].plot(unique, counts)\n",
    "axes[1].plot(bin_centers, histogram)\n",
    "axes[1].legend(['unique', 'soft'])\n",
    "axes[1].set_title('Actual vs. soft estimated distributions - H={:.2f} vs. {:.2f}'.format(entropy_estimate, entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_min = np.percentile(batch_z.astype(np.int32), 0.1)\n",
    "q_max = np.percentile(batch_z.astype(np.int32), 100 - 0.1)\n",
    "\n",
    "bin_boundaries = np.arange(q_min + 0.5, q_max - 0.5, 1)\n",
    "bin_centers = np.convolve(bin_boundaries, [0.5, 0.5], mode='valid')\n",
    "\n",
    "# print(bin_boundaries)\n",
    "\n",
    "\n",
    "batch_z = dcn.compress(batch_x)\n",
    "# Compute frequencies using a histogram\n",
    "freq = np.histogram(batch_z[:], bins=bin_boundaries, normed=False)[0]\n",
    "hist = np.histogram(batch_z[:], bins=bin_boundaries, normed=True)[0]\n",
    "hist = hist.clip(1e-9)\n",
    "probs = hist / np.sum(hist)\n",
    "\n",
    "# Compute frequencies using unique\n",
    "indices, _ = vq(batch_z.reshape((-1, )), bin_centers)\n",
    "unique, counts = np.unique(bin_centers[indices], return_counts=True)\n",
    "\n",
    "print('Bin centers - {} values'.format(len(bin_centers)))\n",
    "for bin, value in zip(bin_centers, freq):\n",
    "    print('  ',bin, '->', value.round(2))   \n",
    "\n",
    "# print(hist.round(2))\n",
    "# print(probs.round(2))\n",
    "\n",
    "entropy = np.sum(- probs * np.log2(probs))\n",
    "\n",
    "print('Naive coding: {:.2f}'.format(np.log2(len(bin_centers))))\n",
    "print('Entropy: {:.2f}'.format(entropy))\n",
    "\n",
    "from dahuffman import HuffmanCodec\n",
    "\n",
    "codec = HuffmanCodec.from_frequencies({k: v for k, v in zip(bin_centers.astype(np.int), freq)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in zip(bin_centers.astype(np.int), probs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word codec.get_code_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq\n",
    "\n",
    "print(batch_z.T[0].shape)\n",
    "\n",
    "code_book = bin_centers.astype(np.int)\n",
    "\n",
    "# Vector quantization\n",
    "indices, distortion = vq(batch_z.T[0], code_book)\n",
    "batch_q = code_book[indices]\n",
    "\n",
    "print('Codebook {}: {}'.format(len(code_book), code_book))\n",
    "print(batch_z.T[0])\n",
    "print(batch_q)\n",
    "print(indices)\n",
    "\n",
    "encoded = codec.encode(batch_q)\n",
    "bits_per_symbol = np.ceil(np.log2(len(bin_centers)))\n",
    "\n",
    "print('Naive coding: {:.0f} bytes'.format(bits_per_symbol * len(batch_q) / 8))\n",
    "print('Theoretical limit: {:.0f} bytes'.format(entropy * len(batch_q) / 8))\n",
    "print('Compressed (Huffman): {} bytes'.format(len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codec.print_code_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_dirname = os.path.join('./data/raw/compression/', dcn.short_name(), 'raise')\n",
    "dcn.load_model(model_output_dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plotting\n",
    "\n",
    "# dcn.init()\n",
    "\n",
    "batch_x = data.next_validation_batch(0, 256)\n",
    "\n",
    "# See latent distribution\n",
    "# batch_z = dcn.compress(batch_x).T\n",
    "batch_z = dcn.compress(batch_x).reshape((256, -1)).T\n",
    "# batch_z = batch_x.reshape((256, 128*128*3)).T\n",
    "print(batch_z.shape)\n",
    "\n",
    "# cov = batch_z.T * batch_z\n",
    "\n",
    "# cov = np.cov(batch_z)\n",
    "\n",
    "cov = np.corrcoef(batch_z)\n",
    "\n",
    "plt.imshow(cov)\n",
    "\n",
    "# batch_z = batch_z[:9]\n",
    "\n",
    "# fig, axes = plotting.sub(len(batch_z) + 1, ncols=10, figwidth=20)\n",
    "\n",
    "# # print(axes)\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "   \n",
    "#     if i >= len(batch_z):\n",
    "#         plotting.quickshow(cov)\n",
    "#     else:        \n",
    "#         ax.hist(batch_z[i], bins=30)\n",
    "#         ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_centers = [-3, -2, -1, 0, 1, 2, 3]\n",
    "p, x = np.histogram(np.random.normal(size=(1000,)), bins=bin_centers)\n",
    "print(bin_centers)\n",
    "print(x)\n",
    "print(np.convolve(x, [0.5, 0.5], mode='valid'))\n",
    "print(p)\n",
    "plt.plot(np.convolve(x, [0.5, 0.5], mode='valid'), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_means = np.mean(batch_z,axis=1)\n",
    "fig, axes = plotting.sub(2, figwidth=12)\n",
    "axes[0].plot(latent_means)\n",
    "axes[0].set_title('Mean values for all latent variables')\n",
    "axes[1].hist(latent_means, bins=50, normed=True)\n",
    "axes[1].set_title('Histogram of mean values for all latent variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x = data.next_validation_batch(0, 400)\n",
    "\n",
    "batch_z = dcn.compress(batch_x)\n",
    "batch_z = batch_z.reshape((sample_batch_size, -1)).T\n",
    "print(batch_z.shape)\n",
    "\n",
    "n_bins = 64\n",
    "bin_boundaries = np.linspace(-16, 16, n_bins+1)\n",
    "bin_centers = np.convolve(bin_boundaries, [0.5, 0.5], mode='valid')\n",
    "\n",
    "print(bin_centers)\n",
    "\n",
    "distribution = np.zeros((len(batch_z), n_bins))\n",
    "for i in range(len(batch_z)):\n",
    "    distribution[i, :] = np.histogram(batch_z[i], bins=bin_boundaries, normed=True)[0]\n",
    "\n",
    "vis = []\n",
    "for i in range(len(distribution) // 512):\n",
    "    vis.append(distribution[i*512:(i+1)*512, :])\n",
    "    \n",
    "\n",
    "thumbs = plotting.thumbnails(vis, n_cols=len(vis))\n",
    "    \n",
    "fig = plotting.imsc(thumbs, 'A', figwidth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 64\n",
    "bin_boundaries = np.linspace(-256, 256, n_bins+1)\n",
    "bin_centers = np.convolve(bin_boundaries, [0.5, 0.5], mode='valid')\n",
    "\n",
    "i = 58\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "fig.gca().fill_between(bin_centers, 0, np.histogram(batch_z[i], bins=bin_boundaries, normed=True)[0], alpha=0.1)\n",
    "print(distribution[i, :])\n",
    "print(batch_z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 4))\n",
    "for i in range(len(batch_z)):\n",
    "#     fig.gca().plot(bin_centers, distribution[i, :])\n",
    "    fig.gca().fill_between(bin_centers, 0, distribution[i, :], alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in dcn.sess.graph.get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq\n",
    "\n",
    "code_book = np.array([-1, 0, 1])\n",
    "\n",
    "batch_x = data.next_validation_batch(0, 50)\n",
    "\n",
    "alpha = 1\n",
    "n_unique = 64\n",
    "\n",
    "# See latent distribution\n",
    "# batch_z = dcn.compress(batch_x).T\n",
    "batch_z = dcn.compress(batch_x)\n",
    "# print(batch_z[0, 0:15].round(3))\n",
    "# batch_z = batch_z.clip(-100, 100)\n",
    "# batch_z = (alpha * batch_z).astype(np.int32) / alpha\n",
    "# batch_z = ((alpha * batch_z).astype(np.int32).clip(-n_unique // 2 + 1, n_unique // 2) / alpha)\n",
    "# print(batch_z[0, 0:15])\n",
    "print('Int from {} to {}'.format(-n_unique // 2 + 1, n_unique // 2))\n",
    "print('  unique = {}'.format(np.unique(batch_z)))\n",
    "print('# unique = {}'.format(len(np.unique(batch_z))))\n",
    "\n",
    "# Vector quantization\n",
    "# indices, distortion = vq(batch_z, code_book)\n",
    "# batch_z = code_book[indices]\n",
    "\n",
    "batch_y = dcn.decompress(batch_z)\n",
    "\n",
    "f = plotting.imsc(batch_x[0:8], ncols=8, figwidth=20)\n",
    "f = plotting.imsc(batch_y[0:8], ncols=8, figwidth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_id = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.gca().hist(batch_z[dim_id])\n",
    "print(batch_z[dim_id].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn.save_model('./data/raw/compression/aedcn/8x8x192', epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Entropy Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_dcn import restore_model\n",
    "from dahuffman import HuffmanCodec\n",
    "\n",
    "dirname = './data/raw/compression_scenarios_final/{}/autoencoderdcn/'\n",
    "\n",
    "dcn_model = 'AutoencoderDCN-8192D/16x16x32-3C2R+BN-r:identity-Q-5.0bpf-S-'\n",
    "\n",
    "dcn = restore_model(dirname.format(dcn_model), patch_size=256)\n",
    "\n",
    "print(dcn._h.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load an example image\n",
    "\n",
    "image = imageio.imread('./data/clic256/alberto-montalesi-176097.png').astype(np.float32) / (2**8 - 1)\n",
    "# image = resize(image, (dcn.patch_size, dcn.patch_size))\n",
    "batch_x = np.expand_dims(image, axis=0)\n",
    "# batch_x = utils.slidingwindow(image, 128)\n",
    "# batch_x = batch_x[6:7]\n",
    "fig = plotting.imsc(batch_x, figwidth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.histogram(batch_z.reshape((-1,1)), bins=code_book_edges, density=True)[0]\n",
    "\n",
    "print(code_book.tolist())\n",
    "print(code_book_edges.tolist())\n",
    "print(counts)\n",
    "\n",
    "plt.plot(code_book, np.ones_like(code_book), 'x')\n",
    "plt.plot(code_book_edges[1:-1], np.ones_like(code_book_edges[1:-1]), '|')\n",
    "\n",
    "plt.plot(code_book, counts, '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density = True\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.hist(batch_z.reshape((-1,1)), bins=code_book, density=density, alpha=0.5)\n",
    "plt.hist(batch_z.reshape((-1,1)), bins=500, density=density, alpha=0.5)\n",
    "plt.plot(code_book, np.histogram(batch_z.reshape((-1,1)), bins=utils.bin_egdes(code_book), density=density)[0], 'k-')\n",
    "plt.plot(code_book, utils.qhist(batch_z, code_book, density=density), 'o--')\n",
    "plt.legend(['hist/codebook', 'hist/uniform', 'np.hist/codebook', 'utils.hist/codebook'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare the DCN and JPEG\n",
    "\n",
    "batch_z = dcn.compress(batch_x, is_training=False)\n",
    "batch_d = dcn.process(batch_x, is_training=False)\n",
    "\n",
    "if dcn._h.rounding == 'identity':\n",
    "    code_book = np.array(code_books[32]).reshape((-1,))\n",
    "#     code_book = np.linspace(-2, 2, 256)\n",
    "else:\n",
    "    code_book = dcn.sess.run(dcn.codebook).reshape((-1,))\n",
    "    code_book_edges = np.convolve(code_book, [0.5, 0.5], mode='valid')\n",
    "\n",
    "indices, _ = vq(batch_z.reshape((-1, )), code_book)\n",
    "print('Codebook size: {} // {}'.format(len(code_book), code_book.tolist()))\n",
    "counts = utils.qhist(batch_z, code_book)\n",
    "\n",
    "counts = counts.clip(min=1)\n",
    "probs = counts / counts.sum()\n",
    "entropy = - np.sum(probs * np.log2(probs))\n",
    "\n",
    "# Plot distributions --------------------------------------------------------------------------------------\n",
    "density = True\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(batch_z.reshape((-1,1)), bins=code_book, density=density, alpha=0.5)\n",
    "plt.hist(batch_z.reshape((-1,1)), bins=500, density=density, alpha=0.5)\n",
    "plt.plot(code_book, np.histogram(batch_z.reshape((-1,1)), bins=utils.bin_egdes(code_book), density=density)[0], 'k-')\n",
    "plt.plot(code_book, utils.qhist(batch_z, code_book, density=density), 'o--')\n",
    "plt.legend(['hist/codebook', 'hist/uniform', 'np.hist/codebook', 'utils.hist/codebook'])\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Construct a Huffman codec\n",
    "codec = HuffmanCodec.from_frequencies({k: v for k, v in zip(range(len(code_book)), counts)})\n",
    "\n",
    "# Vector quantization\n",
    "indices, distortion = vq(batch_z.reshape((-1)), code_book)\n",
    "batch_q = code_book[indices]\n",
    "batch_y = dcn.decompress(batch_q.reshape(dcn.latent_shape))\n",
    "\n",
    "coded_image = codec.encode(indices.tolist())\n",
    "\n",
    "print(batch_x.min(), batch_x.max())\n",
    "print(batch_y.min(), batch_y.max())\n",
    "print(batch_d.min(), batch_d.max())\n",
    "\n",
    "ssim_value = ssim(batch_x[0], batch_y[0], multichannel=True, data_range=1)\n",
    "ssim_value_direct = ssim(batch_x[0], batch_d[0], multichannel=True, data_range=1)\n",
    "\n",
    "# ssim_value = msssim(batch_x[0], batch_y[0])\n",
    "# ssim_value_direct = msssim(batch_x[0], batch_d[0])\n",
    "\n",
    "coded_fse = pyfse.easy_compress(bytes(indices.astype(np.uint8)))\n",
    "\n",
    "print('DCN             : {}'.format(dcn.model_code))\n",
    "print('Pixels          : {}x{} = {:,}'.format(image.shape[0], image.shape[1], np.prod(image.shape[:2])))\n",
    "print('Bitmap          : {:,} bytes'.format(np.prod(image.shape)))\n",
    "print('Batch size      : {:,} elements'.format(np.prod(batch_x.shape)))\n",
    "print('Code-book size  : {} elements'.format(len(code_book)))\n",
    "print('Entropy         : {:.2f} bits per symbol'.format(entropy))\n",
    "print('Latent size     : {:,}'.format(np.prod(batch_z.shape)))\n",
    "print('PPF Naive       : {:,.0f} --> {:,.0f} bytes [{} bits per element]'.format(\n",
    "    np.prod(batch_z.shape) * np.log2(len(code_book)) / 8,\n",
    "    np.prod(batch_z.shape) * np.ceil(np.log2(len(code_book))) / 8,\n",
    "    np.ceil(np.log2(len(code_book)))\n",
    "    ))\n",
    "print('PPF Theoretical : {:,.0f} bytes ({:.2f} bpp)'.format(np.prod(batch_z.shape) * entropy / 8, \n",
    "                                                            np.prod(batch_z.shape) * entropy / np.prod(batch_x.shape[1:3])))\n",
    "print('PPF Huffman     : {:,} bytes ({:.2f} bpp) --> ssim: {:.3f}'.format(len(coded_image), \n",
    "                                                              8 * len(coded_image) / np.prod(batch_x.shape[1:3]),\n",
    "                                                              ssim_value))\n",
    "print('FSE Coded       : {:,.0f} bytes ({:.2f} bpp)'.format(len(coded_fse), \n",
    "                                                            8 * len(coded_fse) / np.prod(batch_x.shape[1:3])))\n",
    "\n",
    "try:\n",
    "    jpeg_quality = jpeg_helpers.match_ssim(batch_x[0], ssim_value, subsampling='4:4:4')\n",
    "except:\n",
    "    jpeg_quality = 95\n",
    "    \n",
    "# jpeg_quality = 75\n",
    "\n",
    "# Encode JPEG\n",
    "# imageio.imwrite('test.jpg', batch_x[0], quality=jpeg_quality, subsampling='4:4:4')\n",
    "batch_j, jpeg_bytes = jpeg_helpers.compress_batch(batch_x, jpeg_quality, effective=True, subsampling='4:4:4')\n",
    "ssim_value_jpeg = ssim(batch_x[0], batch_j[0], multichannel=True, data_range=1)\n",
    "# ssim_value_jpeg = msssim(batch_x[0], batch_d[0])\n",
    "jpeg_bytes = jpeg_bytes[0]\n",
    "\n",
    "print('JPEG (Q={:2d})     : {:,} bytes ({:0.2f} bpp) --> ssim: {:.3f}'.format(\n",
    "    jpeg_quality, jpeg_bytes, 8 * jpeg_bytes / np.prod(batch_j.shape[1:3]), ssim_value_jpeg ))\n",
    "\n",
    "example_id = 0\n",
    "batch_all = np.concatenate((batch_x[example_id:example_id+1], batch_d[example_id:example_id+1], batch_y[example_id:example_id+1], batch_j[example_id:example_id+1]), axis=0)\n",
    "\n",
    "plot_titles = ['Original', 'DCN direct ({:.2f})'.format(ssim_value_direct), 'DCN codec ({:.3f})'.format(ssim_value), 'JPEG Q={} ({:.3f})'.format(jpeg_quality, ssim_value_jpeg)]\n",
    "f = plotting.imsc(batch_all, titles=plot_titles, ncols=len(batch_all)/2, figwidth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = utils.qhist(batch_z, code_books[bins])\n",
    "cc = plt.hist(batch_z.reshape((-1,1)), bins=50)\n",
    "\n",
    "cc = np.histogram(batch_z.reshape((-1,1)))[0]\n",
    "\n",
    "print(sum(cc))\n",
    "print(sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(code_book)\n",
    "np.trapz(np.ones_like(code_book), code_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 32\n",
    "plt.hist(batch_z.reshape((-1,1)), bins=bins, density=True)\n",
    "plt.plot(code_books[bins], utils.qhist(batch_z, code_books[bins], density=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_book = sorted(quantization.generate_codebook(\n",
    "    np.abs(batch_z.reshape((-1,1))[::10]), \n",
    "    size_codebook=bins\n",
    ")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.sort(np.concatenate((-np.array(code_book), np.zeros((1,1)), np.array(code_book)), axis=0).reshape(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(code_book, np.ones_like(code_book), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_zero = False\n",
    "\n",
    "code_books = {}\n",
    "for bins in [4, 8, 16, 32, 64, 128, 256]:\n",
    "    print(bins)\n",
    "    \n",
    "    code_book = sorted(quantization.generate_codebook(\n",
    "        np.abs(batch_z.reshape((-1,1))[::10]), \n",
    "        size_codebook=bins//2\n",
    "    )[0])\n",
    "    \n",
    "    if add_zero:\n",
    "        code_books[bins] = np.sort(np.concatenate((-np.array(code_book[:-1]), np.zeros((1,1)), np.array(code_book)), axis=0).reshape(-1,))\n",
    "    else:\n",
    "        code_books[bins] = np.sort(np.concatenate((-np.array(code_book), np.array(code_book)), axis=0).reshape(-1,))\n",
    "        \n",
    "    print(len(code_books[bins]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bins in [4, 8, 16, 32, 64, 128, 256]:\n",
    "    plt.plot(code_books[bins], 'o-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{')\n",
    "for k, v in code_books.items():\n",
    "    pp = ', '.join(['{:.6f}'.format(x) for x in v])\n",
    "    print('  {}: np.array([{}]),'.format(k, pp))\n",
    "print('}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 32\n",
    "plt.figure(figsize=(20,2))\n",
    "plt.plot(code_books[bins], np.ones_like(code_books[bins]), 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    batch_z.reshape((-1,1)),\n",
    "    batch_q.reshape((-1,1)),\n",
    "    '.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compression import afi\n",
    "\n",
    "out_a, out_b = afi.dcn_compare(dcn, batch_x)\n",
    "\n",
    "diff = np.abs(out_a - out_b)\n",
    "diff /= diff.max()\n",
    "\n",
    "fig = plotting.imsc((out_a, out_b, diff), ncols=3, figwidth=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_p = data.next_validation_batch(0, 5)\n",
    "\n",
    "batch_g = utils.batch_gamma(batch_p)\n",
    "\n",
    "fig = plotting.imsc(batch_p, ncols=5, figwidth=20)\n",
    "fig = plotting.imsc(batch_g, ncols=5, figwidth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to package everything into a file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_struct = {\n",
    "    'height': batch_x.shape[0],\n",
    "    'width': batch_y.shape[1],\n",
    "    'table': codec.get_code_table(),\n",
    "    'data': coded_image\n",
    "}\n",
    "\n",
    "data =pickle.dumps(image_struct)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntropyCoder(object):\n",
    "    \n",
    "    def __init__(self, counts):\n",
    "        pass\n",
    "    \n",
    "    def encode(self, symbols):\n",
    "        pass\n",
    "    \n",
    "    def decode(self, stream):\n",
    "        pass\n",
    "    \n",
    "    def encode_table(self):\n",
    "        pass\n",
    "    \n",
    "    def decode_table(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('data.h5', 'w') as hf:\n",
    "    hf.create_dataset('code', data=b'cjhasb cgukgdbt67xbr3tr78t32ctb6r2')\n",
    "    hf.create_dataset('dataset_2', data=\"{'EOF': 1, '1': 2, '0': 0}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
